{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "To build our Transformer model, we’ll follow these steps:\n",
        "\n",
        "\n",
        "1.   Import necessary libraries and modules\n",
        "2.   Define the basic building blocks: Multi-Head Attention, Position-wise Feed-Forward Networks, Positional Encoding\n",
        "3.   Build the Encoder and Decoder layers\n",
        "4.   Combine Encoder and Decoder layers to create the complete Transformer model\n",
        "5.   Prepare sample data\n",
        "6.   Train the model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pQ500ny6a-5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Importing the necessary libraries and modules**"
      ],
      "metadata": {
        "id": "J8p5PG6Cb69m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import copy"
      ],
      "metadata": {
        "id": "KEMMy9xmbSYl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Defining the basic building blocks: Multi-Head Attention, Position-wise Feed-Forward Networks, Positional Encoding**"
      ],
      "metadata": {
        "id": "tHXJC5LOcOju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Multi-head Attention**"
      ],
      "metadata": {
        "id": "WpXqcbFacaxw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   d_model: Dimensionality of the input.\n",
        "*   num_heads: The number of attention heads to split the input into.\n"
      ],
      "metadata": {
        "id": "g7qBvq2ciucv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        # Initialize dimensions\n",
        "        self.d_model = d_model # Model's dimension\n",
        "        self.num_heads = num_heads # Number of attention heads\n",
        "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
        "\n",
        "        # Linear layers for transforming inputs\n",
        "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
        "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
        "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
        "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        # Calculate attention scores\n",
        "        # swaps the second-to-last dimension with the last dimension K.transpose(-2, -1)\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "            # replace 0 with -1e9\n",
        "\n",
        "        # Softmax is applied to obtain attention probabilities\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        # dim=-1: The softmax is applied along the last dimension (which corresponds to the sequence length of the keys)\n",
        "\n",
        "        # Multiply by values to obtain the final output\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # Reshape the input to have num_heads for multi-head attention\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        # After this operation, each head’s attention mechanism will work with a smaller, independent representation ([batch_size, num_heads, seq_length, d_k])\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        # Combine the multiple heads back to original shape\n",
        "        batch_size, _, seq_length, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        # Apply linear transformations and split heads\n",
        "        Q = self.split_heads(self.W_q(Q))\n",
        "        K = self.split_heads(self.W_k(K))\n",
        "        V = self.split_heads(self.W_v(V))\n",
        "\n",
        "        # Perform scaled dot-product attention\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "        # Combine heads and apply output transformation\n",
        "        output = self.W_o(self.combine_heads(attn_output))\n",
        "        return output"
      ],
      "metadata": {
        "id": "RsGCKiU2cG-W"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Position-wise Feed-Forward Networks**"
      ],
      "metadata": {
        "id": "Czq60hcWsAZo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. d_model: Dimensionality of the model's input and output.\n",
        "2. d_ff: Dimensionality of the inner layer in the feed-forward network.\n",
        "3. self.fc1 and self.fc2: Two fully connected (linear) layers with input and output dimensions as defined by d_model and d_ff.\n",
        "4. self.relu: ReLU (Rectified Linear Unit) activation function, which introduces non-linearity between the two linear layers."
      ],
      "metadata": {
        "id": "7zX2QzKtshvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))"
      ],
      "metadata": {
        "id": "uP_iol11jQQp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Positional Encoding**"
      ],
      "metadata": {
        "id": "hxbLr_qhs8lQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. d_model: The dimension of the model's input.\n",
        "2. max_seq_length: The maximum length of the sequence for which positional encodings are pre-computed.\n",
        "3. pe: A tensor filled with zeros, which will be populated with positional encodings.\n",
        "4. position: A tensor containing the position indices for each position in the sequence.\n",
        "5. div_term: A term used to scale the position indices in a specific way.\n",
        "6. The sine function is applied to the even indices and the cosine function to the odd indices of pe.\n",
        "7. Finally, pe is registered as a buffer, which means it will be part of the module's state but will not be considered a trainable parameter."
      ],
      "metadata": {
        "id": "bv4bhEaHtcoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "hmDMncJdjQT2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Building the Encoder Blocks**"
      ],
      "metadata": {
        "id": "oEM65reCuA4R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. self.self_attn: Multi-head attention mechanism.\n",
        "2. self.feed_forward: Position-wise feed-forward neural network.\n",
        "3. self.norm1 and self.norm2: Layer normalization, applied to smooth the layer's input.\n",
        "4. self.dropout: Dropout layer, used to prevent overfitting by randomly setting some activations to zero during training."
      ],
      "metadata": {
        "id": "IlnGeWQhvROs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "DE23J0VKjQW-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Building the Decoder Blocks**"
      ],
      "metadata": {
        "id": "MFsPXGnqveGX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. x: The input to the decoder layer.\n",
        "2. enc_output: The output from the corresponding encoder (used in the cross-attention step).\n",
        "3. src_mask: Source mask to ignore certain parts of the encoder's output.\n",
        "4. tgt_mask: Target mask to ignore certain parts of the decoder's input."
      ],
      "metadata": {
        "id": "Ewreg7OSwPpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "usa3x5l2jQZp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Combining the Encoder and Decoder layers to create the complete Transformer network**"
      ],
      "metadata": {
        "id": "YHRFe4NhwY5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. self.encoder_embedding: Embedding layer for the source sequence.\n",
        "2. self.decoder_embedding: Embedding layer for the target sequence.\n",
        "3. self.positional_encoding: Positional encoding component.\n",
        "4. self.encoder_layers: A list of encoder layers.\n",
        "5. self.decoder_layers: A list of decoder layers.\n",
        "6. self.fc: Final fully connected (linear) layer mapping to target vocabulary size.\n",
        "7. self.dropout: Dropout layer."
      ],
      "metadata": {
        "id": "dTxJjAiA02Yn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, src, tgt):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
        "        seq_length = tgt.size(1)\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
        "        tgt_mask = tgt_mask & nopeak_mask\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
        "\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "\n",
        "        dec_output = tgt_embedded\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "        output = self.fc(dec_output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "-WUUsvMXwFXr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training the PyTorch Transformer Model**"
      ],
      "metadata": {
        "id": "MAgbNAfj3a96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sample data preparation**"
      ],
      "metadata": {
        "id": "x1EpzXKP3mzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following lines generate random source and target sequences:\n",
        "\n",
        "1. src_data: Random integers between 1 and src_vocab_size, representing a batch of source sequences with shape (64, max_seq_length).\n",
        "2. tgt_data: Random integers between 1 and tgt_vocab_size, representing a batch of target sequences with shape (64, max_seq_length).\n",
        "3. These random sequences can be used as inputs to the transformer model, simulating a batch of data with 64 examples and sequences of length 100."
      ],
      "metadata": {
        "id": "aWAKO4JX4OB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab_size = 5000\n",
        "tgt_vocab_size = 5000\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "max_seq_length = 100\n",
        "dropout = 0.1\n",
        "\n",
        "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
        "\n",
        "# Generate random sample data\n",
        "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
        "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)"
      ],
      "metadata": {
        "id": "21IpMw-U3Sr2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training the Model**"
      ],
      "metadata": {
        "id": "ThgbueSp4ZrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "transformer.train()\n",
        "\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    output = transformer(src_data, tgt_data[:, :-1])\n",
        "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "2z7OZxI34Wc_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Transformer Model Performance Evaluation**"
      ],
      "metadata": {
        "id": "AFkU-w9F5UrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.eval()\n",
        "\n",
        "# Generate random sample validation data\n",
        "val_src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
        "val_tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    val_output = transformer(val_src_data, val_tgt_data[:, :-1])\n",
        "    val_loss = criterion(val_output.contiguous().view(-1, tgt_vocab_size), val_tgt_data[:, 1:].contiguous().view(-1))\n",
        "    print(f\"Validation Loss: {val_loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7x0wvCY5M_K",
        "outputId": "c52c5231-e223-43b9-9e68-875c93a819b9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 8.676915168762207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2ecU7f_l6BAM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}